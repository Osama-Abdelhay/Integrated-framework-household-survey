---
title: "Analysis Scripts"
---

```{r options_communes, include=FALSE}
source("options_communes.R")
```

<div class="note">
 **Key Take Away** :
</div>

## Defining Script for Analysis Automation

Using the right combination of packages fromt he R statistical language, it is possible to  integrate all necessary data analysis steps into **scripts**:

 * Data management (clean, recode, merge, reshape)

 * Data analysis (test, regression, multivariate analysis, etc...)
 
 * Data visualisation (plot, map, graph...)

 * Writing up results (report and presentation generation)

![](images/analysis.png)

This allow for [reproducible data analysis workflow](http://edouard-legoupil.github.io/humanitaRian-data-science/slides/)

### Categorical variables having several code options:

 * In the data cleaning process, make sure that the codes are the same as those on the standard questionnaire. E.g. vitamin A should be coded 1-3 in the questionnaire but code ‘4’ may appear in the database. This code ‘4’ needs to be excluded from analysis and not included in the denominator as it was a mistake in data recording on the field.

Code ‘8 or ‘98’ for ‘don’t know’:

 * Make sure to exclude codes ‘8’, ‘98’ or ‘998’ for ‘don’t know’ from the analysis. This should not be part of the denominator in the calculation of indicators (e.g. diarrhoea, IYCF, ANC enrolment).

Code ‘6’ or ‘96’ for ‘other’:

 * Make sure to include codes ‘6’ or ‘96’ for ‘other’ in the analysis. This should be part of the denominator in the calculation of indicators as it represents one response option (e.g. safe disposal of U3 stool, reason for not having a ration card, source of water).

Recoding with the ‘if’ command in Epi Info software:

 * Do not forget to take into account missing values when using the ‘if’ command. This applies to many different variables. Whenever feasible, it is much better to use the ‘recode’ command instead of the ‘if’ command for this specific reason.

### Confidence intervals:

 * Different software often calculate confidence intervals as being negative and hence below zero or above 100. Negative confidence intervals or CI above 100 are meaningless. In the report, always round negative confidence intervals to ‘0’ and round those above 100 to ‘100’.

 
### Rounding decimal points:

 * Make sure to round properly decimal points according to basic rules:
 *  When decimal is between 1-4, round down.
 *  When decimal is between 5-9, round up.

### Decimal points in the results:

 * When the results is a whole number e.g. 30%, make sure to always write 30.0% with the ‘.0’ in the decimal place in the report. This ensures that the decimal point was not forgotten and is actually equal to zero.

### Missing data or consent not provided:

 * Data should be excluded from all analysis and should not be accounted for in the denominator.


### Always clean the data first before going into analysis:

 * Frequencies and means should be run on categorical and continuous variables, respectively.
 * Missing data should be looked at and a record of them should be kept.

### Age variable:

 * When selecting age or creating an age variable category in Epi info software from the ‘months’ variable generated by ENA, don’t forget the ‘.99’ otherwise some children with an exact birth date may be excluded from the analysis. E.g. 6-23.99 (and not 6-23 or 6-23.9).

### Reproducible Analysis

Always save newly generated variables into a new data file named following a naming convention to be respected by all involved in the survey data analysis.





## Analysis Type

### <a href="http://schmidheiny.name/teaching/ols.pdf" target=blank> The Multiple Linear Regression Model </a>
The multiple linear regression model and its estimation using ordinary least squares (OLS) is doubtless the most widely used tool in econometrics. It allows to estimate the relation between a dependent variable and a set of explanatory variables minimizing the squared distances between the observed and the predicted dependent variable. <br>

__Key concepts__: OLS Assumptions and Estimation, Goodness of Fit, Small Sample Properties, Tests in Small Samples, Confidence Intervals in Small Sample, Asymptotic Properties of the OLS Estimator, Asymptotic Tests, Confidence Intervals in Large Samples, Small Sample vs. Asymptotic Properties, More Known Issues (Non-linear Functional Form, Aggregate Regressors, Omitted Variables, Irrelevant Regressors, Reverse Causality, Measurement Error, Mutlicollinearity). 

### <a href="http://schmidheiny.name/teaching/functionalform.pdf" target=blank> Functional Form in the Linear Model </a>
Despite its name, the classical _linear_ regression model, is not limited to a linear relationship between the dependent and the explanatory variables. It is indeed possible to build a model which is linear in the parameters but that also includes non-linear functions of the regressors. <br>

__Key concepts__: Log-Linear, Semi-Log, Polynomial, Inverse, Dummy Variables, Interaction Terms, Spline Functions.

### <a href="http://schmidheiny.name/teaching/heteroscedasticity.pdf" target=blank> Heteroskedasticity in the Linear Model </a>
This chapter relaxes the homoscedasticity assumption of the least squares estimation, and shows how the parameters of the linear model can be correctly estimated and tested when the error terms are heteroscedastic, i.e. their variance, conditioned on the regressors, changes across observations. <br>

__Key concepts__: Groupwise Heteroskedasticity, Estimation with OLS, Estimating the Variance of the OLS Estimator, Testing for Heteroskedasticity, Estimation with GLS/WLS when the Variance Matrix is Known, Estimation with FGLS/FWLS when the Variance Matrix is Unknown.

### <a href="http://schmidheiny.name/teaching/clustering.pdf" target=blank> Clustering in the Linear Model </a>
This chapter relaxes the homoscedasticity assumption of the least squares estimation and allows the error terms to be heteroscedastic and correlated within groups or so-called clusters. It shows in what situations the parameters of the linear model can be consistently estimated by OLS and how the standard errors need to be corrected. Clustering might arise when the sampling mechanism first draws a random sample of groups (e.g. schools, households, towns) and then surveys all (or a random sample of) observations within that group. <br>

__Key concepts__: Random Cluster-Specific Effects, Estimation with OLS, Estimating Correct Standard Errors, Efficient Estimation with GLS, Estimating Correct Standard Errors with Random Cluster-Specific Effects.

### <a href="http://schmidheiny.name/teaching/iv.pdf" target=blank> Instrumental Variables </a>
In many applications of the linear model, we suspect that some regressors are endogenous, i.e. one or more regressors are correlated with the error term. In this situation, OLS cannot consistently estimate the causal effect of the regressor on the dependent variable. Sometimes, we are able to find exogenous variables which are correlated with the endogenous regressor but not correlated with the error term. Such variables are called instrumental variables or instruments. If there are enough good such instruments, we can estimate the causal effect of the regressor on the dependent variable. <br>

__Key concepts__: Canonical Examples (Omitted Variables, Simultaneity and Reversed Causality, Measurement Errors), Estimation with OLS, Estimation with IV (2SLS), Asymptotic Properties of the IV Estimators, What are Valid Instruments, Testing for Exogeneity of the Instruments, Testing for Relevance of the Instruments, Testing for Exogeneity of the Regressors.

### <a href="http://schmidheiny.name/teaching/panel.pdf" target=blank> Panel Data: Fixed and Random Effects </a>
In panel data, individuals (persons, firms, cities, ... ) are observed at several points in time (days, years, before and after treatment, ...). This chapter focuses on panels with relatively few time periods (small $T$) and many individuals (large $N$). This chapter introduces the two basic models for the analysis of panel data, the fixed effects model and the random effects model, and presents consistent estimators for these two models. Panel data are most useful when we suspect that the outcome variable depends on explanatory variables which are not observable but correlated with the observed explanatory variables. If such omitted variables are constant over time, panel data estimators allow to consistently estimate the effect of the observed explanatory variables. <br>

__Key concepts__: The Random Effects Model, The Fixed Effects Model, Estimation with Pooled OLS, Random Effects Estimation, Fixed Effects Estimation, Leas Squared Dummy Variable Estimation (LSDV), First Difference Estimator, Time Fixed Effects, Random Effects vs. Fixed Effects Estimation.

### <a href="http://schmidheiny.name/teaching/binaryresponse.pdf" target=blank> Binary Response Models </a>
Many dependent variables of interest in economics and other social sciences can only take two values. The two possible outcomes are usually denoted by 0 and 1. Such variables are called dummy variables or dichotomous variables. As already seen in the course _Introductory Econometrics_, there are several ways to model these outcomes using regressions. This chapter specifically focuses on the interpretation of the Probit and Logit models, and of their estimated parameters. <br>

__Key concepts__: The Econometric Model: Probit and Logit, Latent Variable Model, Interpretation of the Parameters, Estimation with Maximum Likelihood, Estimation with OLS.

### <a href="http://schmidheiny.name/teaching/limiteddependent2up.pdf" target=blank> Limited Dependent Variable Models </a>
* The effect of _truncation_ occurs when the observed data in the sample are only drawn from a subset of a larger population. The sampling of the subset is based on the value of the dependent variable. <br>
* _Censoring_ occurs when the values of the dependent variable are restricted to a range of values. As in the case of truncation the dependent variable is only observed for a subsample. However, there is information (the independent variables) about the whole sample. <br>
* The _sample selection problem_ occurs when the observed sample is not a random sample but systematically chosen from the population. Truncation and censoring are special cases of sample selection or incidental truncation. <br> 

This chapter presents the econometric models that are used to deal with the above-mentioned situations. <br>

__Key concepts__: Truncation, Truncated Regression, Interpretation of Parameters, Estimation; Censoring, Tobit Model Type I, Interpretation of Parameters, Estimation; Selection, Heckman Selection Model, Interpretation of Parameters, Estimation, Estimation with Maximum Likelihood, Estimation with Heckman's Two-Step Procedure.

### Quantile Regression
Quantile regression provides an alternative to ordinary least squares (OLS) regression and related methods, which typically assume that associations between independent and dependent variables are the same at all levels. Quantile methods allow the analyst to relax the common regression slope assumption. In OLS regression, the goal is to minimize the distances between the values predicted by the regression line and the observed values. In contrast, quantile regression differentially weights the distances between the values predicted by the regression line and the observed values, then tries to minimize the weighted distances. <br>
Two empirical applications are here attached to understand why and when it may be appropriate to use this model:

* <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4054530/" target=blank> Thinking beyond the mean: a practical guide for using quantile regression methods for health services research </a>
* <a href="https://www.fort.usgs.gov/publication/21137" target=blank> A gentle introduction to quantile regression for ecologists </a>

### <a href="https://www.otexts.org/fpp/8" target=blank> Advanced Time Series Analysis </a>
Additionally to the forecasting models presented in the chapter _Introduction to Time Series Regression and Forecasting_ of the previous course, there are other time series regressions used for forecasting which involve lags of both the dependent variable and the error term, so called AR(I)MA. Before introducing these models, the concept of stationarity and the technique of differencing time series are discussed. The course also includes applications in R. <br>

__Key concepts__: Stationarity and Differencing, Autoregressive Models, Moving Average Models, Non-seasonal AR(I)MA Models, Estimation and Order Selection, AR(I)MA Modelling in R, Forecasting, Seasonal AR(I)MA Model.

### <a href="http://www.stat.berkeley.edu/~mgoldman/Section0402.pdf" target=blank> Multiple Hypothesis Testing </a>
When a set of hypotheses are tested simultaneously and independently from each other, then the probability of rejecting at least one of the true null hypothesis can become excessively high. Methods for dealing with multiple testing frequently call for adjusting the significance level in some way, so that the probability of observing at least one significant result due to chance remains below your desired significance level. <br>

__Key concepts__: the Problem of Multiple Testing, Bonferroni Correction, False Discovery Rate, Comparison of the Correction Methods.

For a further understanding of this topic, have a look at <a href="http://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/" target=blank> this interactive reading</a>.

